{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bc455a8",
   "metadata": {},
   "source": [
    "# All Functions Used For Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bea4953",
   "metadata": {},
   "source": [
    "## K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "880498ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_kmeans(full_data, k, weight, seed = 3):\n",
    "    data = [list(full_data['LON']), list(full_data['LAT']), list(full_data[weight])]\n",
    "    random.seed(seed)\n",
    "    n = len(data[0]) # number of census tracts\n",
    "    stop = False\n",
    "    labels = np.zeros(n) # assigns each centroid to a cluster\n",
    "    centroids = np.zeros([k, 2]) # centroid for each cluster\n",
    "    count = 0\n",
    "\n",
    "    # randomly initialize centroids to be somewhere in the STL City region (between min and max of data)\n",
    "    '''\n",
    "    for i in range(k):\n",
    "        centroids[i][0] = round(random.uniform(np.min(data[0]), np.max(data[0])), 6)\n",
    "        centroids[i][1] = round(random.uniform(np.min(data[1]), np.max(data[1])), 6)\n",
    "    '''\n",
    "    # randomly initialize centroids to be existing census tracts\n",
    "    for i in range(k):\n",
    "        new_tract = random.randint(0,n-1)\n",
    "        centroids[i][0] = data[0][new_tract]\n",
    "        centroids[i][1] = data[1][new_tract]\n",
    "        \n",
    "    # iterate until labels do not change (or at least twice)\n",
    "    while stop == False or count < 2:\n",
    "        count = count + 1\n",
    "        old_labels = labels\n",
    "\n",
    "        # Calculating labels by finding nearest centroid for each census tract\n",
    "        for i in range(n): # for each tract\n",
    "            closest_distance = float('inf')\n",
    "            tract_center = np.array([data[0][i], data[1][i]])\n",
    "            for j in range(k): # for each centroid/cluster\n",
    "                centroid = np.array([centroids[j][0], centroids[j][1]])\n",
    "                dist = np.linalg.norm(tract_center - centroid)\n",
    "                if (dist < closest_distance):\n",
    "                    closest_distance = dist\n",
    "                    labels[i] = j\n",
    "\n",
    "        # Check if any of the labels changed\n",
    "        all_same = True\n",
    "        for i in range(n):\n",
    "            if (labels[i] != old_labels[i]):\n",
    "                all_same = False\n",
    "        if all_same == True:\n",
    "            stop = True\n",
    "\n",
    "        # updating centroid locations as the (weighted?) mean of each census tract in its cluster\n",
    "        new_centroids = np.zeros([k,2])\n",
    "        for i in range(k): # for each cluster\n",
    "            weights = []\n",
    "            cluster = []\n",
    "            for j in range(n): # for each tract\n",
    "                if (labels[j] == i):\n",
    "                    weights.append(data[2][j]) # weights will not all sum to 1 (different number in each cluster)\n",
    "                    cluster.append(np.array([data[0][j], data[1][j]]))\n",
    "\n",
    "            new_x = [point[0] for point in cluster]\n",
    "            new_y = [point[1] for point in cluster]\n",
    "            normalized_weights = np.array(weights)/np.sum(weights)\n",
    "\n",
    "            if len(new_x) > 1: # if there are points in the cluster\n",
    "                new_centroids[i][0] = np.dot(normalized_weights, new_x)\n",
    "                new_centroids[i][1] = np.dot(normalized_weights, new_y)\n",
    "            else:\n",
    "                new_centroids[i][0] = centroids[i][0]\n",
    "                new_centroids[i][1] = centroids[i][1]\n",
    "\n",
    "        centroids = new_centroids\n",
    "        \n",
    "        return centroids, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02eea832",
   "metadata": {},
   "source": [
    "## Linear Programming "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d62d3f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distances_to_nearest_stop(tract_locations, metro_locations):\n",
    "    \n",
    "    shortest_distance = np.zeros(len(tract_locations))\n",
    "    metro_locations = [[metro_locations[i], metro_locations[i + 1]] for i in range(0, len(metro_locations), 2)] # reshape back into 2d array\n",
    "    \n",
    "    for i in range(len(tract_locations)):\n",
    "        cent = np.array([tract_locations['LON'][i],tract_locations['LAT'][i]])\n",
    "        closest_distance = float('inf') # initialize to infinity\n",
    "        for j in range(len(metro_locations)):\n",
    "            metro = np.array(metro_locations[j])\n",
    "            distance = np.linalg.norm(cent-metro)\n",
    "            if (distance < closest_distance):\n",
    "                closest_distance = distance\n",
    "        shortest_distance[i] = closest_distance\n",
    "    \n",
    "    return shortest_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9cb77699",
   "metadata": {},
   "outputs": [],
   "source": [
    "def orth_dist(x):\n",
    "    x = [[x[i], x[i + 1]] for i in range(0, len(x), 2)] # reshape back into 2d array\n",
    "    longs = np.array([s[0] for s in x]).reshape(-1,1)\n",
    "    lats = np.array([s[1] for s in x])\n",
    "    model = LinearRegression().fit(longs,lats)\n",
    "    b = model.intercept_\n",
    "    m = model.coef_[0]\n",
    "    dists = 0\n",
    "    for stop in x:\n",
    "        n = abs(-1*m*stop[0] + stop[1] - b)\n",
    "        d = np.sqrt(m**2+1)\n",
    "        dists = dists + n/d\n",
    "    return dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "994a3adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distances_between_stops(metro_locations):\n",
    "    metro_locations = [[metro_locations[i], metro_locations[i + 1]] for i in range(0, len(metro_locations), 2)] # reshape back into 2d array\n",
    "    shortest_distance = np.zeros(len(metro_locations))\n",
    "    total_distance = 0\n",
    "    for i in range(len(metro_locations)):\n",
    "        metro_current = np.array([metro_locations[i][0],metro_locations[i][1]])\n",
    "        closest_distance = float('inf') # initialize to infinity\n",
    "        for j in range(len(metro_locations)):\n",
    "            if (i != j):\n",
    "                metro_compared = np.array(metro_locations[j])\n",
    "                distance = np.linalg.norm(metro_current-metro_compared)\n",
    "                if (distance < closest_distance):\n",
    "                    closest_distance = distance\n",
    "        total_distance = total_distance + closest_distance\n",
    "    return total_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8c4856a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_to_mean_x(metro_locations):\n",
    "    metro_locations = [[metro_locations[i], metro_locations[i + 1]] for i in range(0, len(metro_locations), 2)] # reshape back into 2d array\n",
    "    longs = np.array([s[0] for s in metro_locations])\n",
    "    x_avg = np.mean(longs)\n",
    "    deviation_from_avg = 0\n",
    "    for i in range(len(metro_locations)):\n",
    "        deviation_from_avg = deviation_from_avg + np.abs(x_avg - metro_locations[i][0])\n",
    "    return deviation_from_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4393d007",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fun(new_stops):\n",
    "    #weights = full_data['race weight']\n",
    "    weights = np.ones(len(full_data))\n",
    "    dist_to_stops = np.dot(weights, distances_to_nearest_stop(full_data, new_stops))\n",
    "    linearity = orth_dist(new_stops) \n",
    "    dist_btw_stops = distances_between_stops(new_stops)\n",
    "    distance_to_mean = distance_to_mean_x(new_stops)\n",
    "    return 5*dist_to_stops + 0*linearity - 1.0*dist_btw_stops + 2*distance_to_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73171cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_programming(full_data):\n",
    "    k = 12\n",
    "    n = len(full_data)\n",
    "    x0 = []\n",
    "    for i in range(k):\n",
    "        new_tract = random.randint(0,n-1)\n",
    "        x0.append(full_data['LON'][new_tract])\n",
    "        x0.append(full_data['LAT'][new_tract])\n",
    "    \n",
    "    lat_bounds = (min(full_data['LAT']),max(full_data['LAT']))\n",
    "    lon_bounds = (min(full_data['LON']),max(full_data['LON']))\n",
    "    bnds = [val for pair in zip([lon_bounds]*12, [lat_bounds]*12) for val in pair]\n",
    "    \n",
    "    result = minimize(fun, x0, bounds=bnds)\n",
    "    all_centroids = result.x\n",
    "    lp_results = [[all_centroids[i], all_centroids[i + 1]] for i in range(0, len(all_centroids), 2)]\n",
    "    return lp_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6ac886b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_from_lp(tract_locations, metro_locations):\n",
    "    \n",
    "    shortest_distance = np.zeros(len(tract_locations))\n",
    "    cluster_ids = np.zeros(len(tract_locations))\n",
    "    \n",
    "    for i in range(len(tract_locations)):\n",
    "        cent = np.array([tract_locations['LON'][i],tract_locations['LAT'][i]])\n",
    "        closest_distance = float('inf') # initialize to infinity\n",
    "        closest_stop = float('inf')\n",
    "        for j in range(len(metro_locations)):\n",
    "            metro = np.array(metro_locations[j])\n",
    "            distance = np.linalg.norm(cent-metro)\n",
    "            if (distance < closest_distance):\n",
    "                closest_distance = distance\n",
    "                closest_stop = j\n",
    "        shortest_distance[i] = closest_distance\n",
    "        cluster_ids[i] = closest_stop\n",
    "      \n",
    "    return cluster_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7697a13",
   "metadata": {},
   "source": [
    "## Modularity Maximization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5fec6290",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mod_max_weighted(graph, k):\n",
    "    tolerance = 1e-5\n",
    "    orig = graph.copy()\n",
    "    to_return = graph.copy()\n",
    "    q = 0\n",
    "    q_new = 0\n",
    "    partition = [orig]\n",
    "    new_partition = []\n",
    "\n",
    "    while (len(partition) < k):\n",
    "        for G in partition:\n",
    "            # compute modularity matrix B\n",
    "            G_nodes = list(G.nodes())\n",
    "            s = np.zeros(len(G_nodes))\n",
    "            A = nx.to_numpy_array(G, weight='weights')\n",
    "            degs = list(dict(G.degree()).values())\n",
    "            B = np.zeros(np.shape(A))\n",
    "            n = len(G_nodes)\n",
    "            m = len(G.edges())\n",
    "            for i in range(n):\n",
    "                for j in range(n):\n",
    "                    B[i][j] = A[i][j] - degs[i]*degs[j]/(2*m)\n",
    "            # find leading eigenvector u of B\n",
    "            val, vec = np.linalg.eig(B)\n",
    "            idx = np.argsort(val)[-1]\n",
    "            largest_vec = vec[:,idx]\n",
    "            \n",
    "            # divide nodes into two groups by sign of leading eigenvector\n",
    "            for i in range(len(largest_vec)):\n",
    "                if largest_vec[i].real < np.mean(largest_vec):\n",
    "                    s[i] = -1\n",
    "                else:\n",
    "                    s[i] = 1\n",
    "            \n",
    "            # create new subgraphs\n",
    "            n1 = [G_nodes[i] for i in range(n) if s[i] == 1]\n",
    "            n2 = [G_nodes[i] for i in range(n) if s[i] == -1]\n",
    "            if len(n1) >= 1:\n",
    "                new_partition.append(G.subgraph(n1))\n",
    "            if len(n2) >= 1:\n",
    "                new_partition.append(G.subgraph(n2))\n",
    "            \n",
    "            # update q_new\n",
    "            q_new = q_new + B.sum()\n",
    "        \n",
    "        \n",
    "        # either stop or re-partition graph\n",
    "        q_new = q_new / (2*m)\n",
    "        if abs(q_new - q) > tolerance and q_new < q:\n",
    "            return partition\n",
    "        \n",
    "        else:\n",
    "            q = q_new\n",
    "            q_new = 0 \n",
    "            partition = new_partition\n",
    "            new_partition = []\n",
    "\n",
    "    if len(partition) > k: # (this happens if k is odd)\n",
    "        num_to_combine = len(partition) - k # we need to re-combine this many graphs to get back to k subgraphs\n",
    "        pairs = [(partition[i], partition[i + 1]) for i in range(0, len(partition), 2)]\n",
    "        to_test = list(itertools.combinations(pairs, num_to_combine))\n",
    "        \n",
    "        # test modularity of all possible combinations to make partition have length k\n",
    "        best_modularity = 0\n",
    "        test_idx = 0\n",
    "        best_test_idx = 0\n",
    "        for test in to_test:\n",
    "            partition_copy = partition.copy()\n",
    "            # remove the pair from the partition and add back in the original before it split into that pair\n",
    "            for pair in test:\n",
    "                partition_copy.remove(pair[0])\n",
    "                partition_copy.remove(pair[1])\n",
    "                nodes_to_find = list(set(pair[0].nodes()).union(pair[1].nodes()))\n",
    "                back_together = orig.subgraph(nodes_to_find)\n",
    "                partition_copy.append(back_together)\n",
    "            # re-compute modularity and compare to best modularity\n",
    "            communities = []\n",
    "            for clust in partition_copy:\n",
    "                nodes_in_clust = {n for n in clust.nodes()}\n",
    "                communities.append(nodes_in_clust)\n",
    "            modularity = nx.community.modularity(orig, communities)\n",
    "            if modularity > best_modularity:\n",
    "                best_modularity = modularity\n",
    "                best_test_idx = test_idx     \n",
    "            test_idx = test_idx + 1\n",
    "        \n",
    "        # choose the test with the best modularity and combine the pairs in that test for the final result\n",
    "        final_partition = partition.copy()\n",
    "        for pair in to_test[best_test_idx]:\n",
    "            final_partition.remove(pair[0])\n",
    "            final_partition.remove(pair[1])\n",
    "            nodes_to_find = list(set(pair[0].nodes()).union(pair[1].nodes()))\n",
    "            back_together = orig.subgraph(nodes_to_find)\n",
    "            final_partition.append(back_together)\n",
    "            \n",
    "    else: # if k was even\n",
    "        final_partition = partition\n",
    "\n",
    "    value_dict = dict()\n",
    "    idx = 0\n",
    "    for sub in final_partition:\n",
    "        for n in sub.nodes():\n",
    "            value_dict[n] = idx\n",
    "        idx = idx + 1\n",
    "    nx.set_node_attributes(to_return, value_dict, 'cluster')\n",
    "    return to_return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474b3e72",
   "metadata": {},
   "source": [
    "# Graph Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f4588907",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_centroid(group):\n",
    "    return MultiPoint(group.geometry.values).centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c8f4d5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_results(res, title, graph_filename = \"capstone.gexf\", centroids_filename = \"data\\Centroids.shp\"):\n",
    "    \n",
    "    graph = nx.read_gexf(\"capstone.gexf\")\n",
    "    colors = [graph.nodes[node]['cluster'] for node in res.nodes()]\n",
    "    labels = [graph.nodes[node]['label'] for node in res.nodes()]\n",
    "    gdf = gpd.GeoDataFrame(graph.nodes(data=True), columns=['geometry', 'attributes'])\n",
    "    # Add colors and labels to the GeoDataFrame\n",
    "    gdf['color'] = colors\n",
    "    gdf['NAME'] = labels\n",
    "\n",
    "    CentroidsInfo = gpd.read_file(centroids_filename)\n",
    "    # Add color information to centroids GeoDataFrame\n",
    "    centroids_gdf = gdf.merge(CentroidsInfo, on='NAME')\n",
    "    centroids_gdf = gpd.GeoDataFrame(centroids_gdf.rename(columns={'geometry_y': 'geometry'}))\n",
    "    centroids_gdf = centroids_gdf.drop(columns=['geometry_x','attributes'])\n",
    "    centroids_gdf = centroids_gdf.to_crs('EPSG:4326')\n",
    "\n",
    "\n",
    "    cent = centroids_gdf.groupby('color').apply(calculate_centroid)\n",
    "    newPlan = gpd.GeoDataFrame(geometry = cent)\n",
    "\n",
    "    centroids_gdf['geometry'] = centroids_gdf['geometry'].apply(lambda x: Point(x))\n",
    "    new_centroids_gdf = gpd.GeoDataFrame(centroids_gdf, geometry='geometry', crs='EPSG:4326')\n",
    "\n",
    "    # Save to file\n",
    "    newPlan.to_file(r\"data\\newPlanClust.shp\", crs=\"EPSG:4326\")\n",
    "    centroids_gdf.to_file(r\"data\\centroidsColoredByCommunity.shp\",crs=\"EPSG:4326\")\n",
    "    \n",
    "    # ------------------------------------- SAVING FILES ABOVE, GRAPHING BELOW --------------------------------------------\n",
    "    \n",
    "    \n",
    "    # Read the tracts shapefile\n",
    "    tracts = gpd.read_file(r\"data\\census tracts\\CensusTractsStl.shp\")\n",
    "\n",
    "    # Plotting the graph and tracts polygons\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "    # Plot tracts polygons\n",
    "    tracts.plot(ax=ax, color='lightgray', edgecolor='black', alpha=0.5)\n",
    "\n",
    "    coloredCentroids = gpd.read_file(r\"data\\centroidsColoredByCommunity.shp\")\n",
    "    color_dict = {0:'lightgreen', 1:'lightblue', 2:'lavender', 3:'orange', 4:'red', 5:'purple', 6:'yellow', 7:'green', 8:'blue', 9:'magenta', 10:'cyan', 11:'gray'}\n",
    "    cmap = ListedColormap([color_dict[i] for i in range(len(color_dict))])\n",
    "\n",
    "    coloredCentroids.head()\n",
    "\n",
    "\n",
    "    # Plot centroids from centroidsColoredByCommunity.shp with colormap\n",
    "    for _, row in coloredCentroids.iterrows():\n",
    "        centroid = row['geometry'].centroid  # Get centroid of the polygon\n",
    "        color = color_dict[row['color']]\n",
    "        ax.plot(centroid.x, centroid.y, marker='o', color=color, markersize=5)  # Use colormap for centroids\n",
    "\n",
    "    mm_centroids = list()\n",
    "    # Plot centroids of polygons from newPlanClust.shp\n",
    "    newPlan = gpd.read_file(r\"data\\newPlanClust.shp\")\n",
    "    for _, row in newPlan.iterrows():\n",
    "        centroid = row['geometry'].centroid  # Get centroid of the polygon\n",
    "        mm_centroids.append([centroid.x, centroid.y])\n",
    "        ax.plot(centroid.x, centroid.y, marker='*', color='black', markersize=15)\n",
    "\n",
    "    ax.set_title(f'Results of {title}')\n",
    "    ax.set_xlabel('Longitude')\n",
    "    ax.set_ylabel('Latitude')\n",
    "    plt.savefig(f'{title}.png')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    to_return = np.array(mm_centroids)\n",
    "    \n",
    "    return to_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0d10e7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_results_lp(res, lp_stops, title, graph_filename = \"capstone.gexf\", centroids_filename = \"data\\Centroids.shp\"):\n",
    "    \n",
    "    graph = nx.read_gexf(\"capstone.gexf\")\n",
    "    colors = [graph.nodes[node]['cluster'] for node in res.nodes()]\n",
    "    labels = [graph.nodes[node]['label'] for node in res.nodes()]\n",
    "    gdf = gpd.GeoDataFrame(graph.nodes(data=True), columns=['geometry', 'attributes'])\n",
    "    # Add colors and labels to the GeoDataFrame\n",
    "    gdf['color'] = colors\n",
    "    gdf['NAME'] = labels\n",
    "\n",
    "    CentroidsInfo = gpd.read_file(centroids_filename)\n",
    "    # Add color information to centroids GeoDataFrame\n",
    "    centroids_gdf = gdf.merge(CentroidsInfo, on='NAME')\n",
    "    centroids_gdf = gpd.GeoDataFrame(centroids_gdf.rename(columns={'geometry_y': 'geometry'}))\n",
    "    centroids_gdf = centroids_gdf.drop(columns=['geometry_x','attributes'])\n",
    "    centroids_gdf = centroids_gdf.to_crs('EPSG:4326')\n",
    "\n",
    "\n",
    "    cent = centroids_gdf.groupby('color').apply(calculate_centroid)\n",
    "    newPlan = gpd.GeoDataFrame(geometry = cent)\n",
    "\n",
    "    centroids_gdf['geometry'] = centroids_gdf['geometry'].apply(lambda x: Point(x))\n",
    "    new_centroids_gdf = gpd.GeoDataFrame(centroids_gdf, geometry='geometry', crs='EPSG:4326')\n",
    "\n",
    "    # Save to file\n",
    "    newPlan.to_file(r\"data\\newPlanClust.shp\", crs=\"EPSG:4326\")\n",
    "    centroids_gdf.to_file(r\"data\\centroidsColoredByCommunity.shp\",crs=\"EPSG:4326\")\n",
    "    \n",
    "    # ------------------------------------- SAVING FILES ABOVE, GRAPHING BELOW --------------------------------------------\n",
    "    \n",
    "    \n",
    "    # Read the tracts shapefile\n",
    "    tracts = gpd.read_file(r\"data\\census tracts\\CensusTractsStl.shp\")\n",
    "\n",
    "    # Plotting the graph and tracts polygons\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "    # Plot tracts polygons\n",
    "    tracts.plot(ax=ax, color='lightgray', edgecolor='black', alpha=0.5)\n",
    "\n",
    "    coloredCentroids = gpd.read_file(r\"data\\centroidsColoredByCommunity.shp\")\n",
    "    color_dict = {0:'lightgreen', 1:'lightblue', 2:'lavender', 3:'orange', 4:'red', 5:'purple', 6:'yellow', 7:'green', 8:'blue', 9:'magenta', 10:'cyan', 11:'gray'}\n",
    "    cmap = ListedColormap([color_dict[i] for i in range(len(color_dict))])\n",
    "\n",
    "    coloredCentroids.head()\n",
    "\n",
    "\n",
    "    # Plot centroids from centroidsColoredByCommunity.shp with colormap\n",
    "    for _, row in coloredCentroids.iterrows():\n",
    "        centroid = row['geometry'].centroid  # Get centroid of the polygon\n",
    "        color = color_dict[row['color']]\n",
    "        ax.plot(centroid.x, centroid.y, marker='o', color=color, markersize=5)  # Use colormap for centroids\n",
    "\n",
    "    # Plot original lp stops\n",
    "    longs = [i[0] for i in lp_stops]\n",
    "    lats = [i[1] for i in lp_stops]\n",
    "    \n",
    "    for stop in range(len(lp_stops)):\n",
    "        ax.plot(longs[stop], lats[stop], marker='*', color='black', markersize=15)\n",
    "\n",
    "\n",
    "    ax.set_title(f'Results of {title}')\n",
    "    ax.set_xlabel('Longitude')\n",
    "    ax.set_ylabel('Latitude')\n",
    "    plt.savefig(f'{title}.png')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7456e3",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "901687b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist_to_nearest_stop_eval(centroid_locations, metro_locations):\n",
    "    shortest_distance = np.zeros(len(centroid_locations))\n",
    "    for i in range(len(centroid_locations)):\n",
    "        cent = np.array([centroid_locations['LON'][i],centroid_locations['LAT'][i]])\n",
    "        closest_distance = float('inf') # initialize to infinity\n",
    "        for j in range(len(metro_locations)):\n",
    "            metro = np.array([metro_locations[j][0],metro_locations[j][1]])\n",
    "            diff_latlon = cent-metro\n",
    "            diff_miles = np.copy(diff_latlon)\n",
    "            diff_miles[0] = 69*diff_latlon[0]\n",
    "            diff_miles[1] = 54.6*diff_latlon[1]\n",
    "            distance = np.linalg.norm(diff_miles)\n",
    "            if (distance < closest_distance):\n",
    "                closest_distance = distance\n",
    "        shortest_distance[i] = closest_distance\n",
    "    \n",
    "    return shortest_distance\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
